# describes the mcp servers to use
mcpServers:
  filesystem:
    type: stdio
    command: npx
    args:
      - -y
      - '@modelcontextprotocol/server-filesystem'
      - .

# describes the llm configuration
llm:
  provider: openai
  model: gpt-4.1-mini
  apiKey: $OPENAI_API_KEY
  maxIterations: 50
  # provider: anthropic
  # model: claude-3-5-haiku-20241022
  # apiKey: $ANTHROPIC_API_KEY
  # maxIterations: 50
  # provider: openrouter
  # model: google/gemini-2.5-pro
  # apiKey: $OPENROUTER_API_KEY
  # maxIterations: 50
  # provider: qwen
  # model: qwen2.5-72b-instruct
  # apiKey: $QWEN_API_KEY
  # baseURL: https://dashscope-intl.aliyuncs.com/compatible-mode/v1
  # maxIterations: 5
  # qwenOptions:
  #   enableThinking: true
  #   thinkingBudget: 1000
  #   temperature: 0.1
  #   top_p: 0.9
# Evaluation LLM configuration (non-thinking model for evaluation step)
# evalLlm:
#   provider: anthropic
#   model: claude-3-7-sonnet-20250219
#   apiKey: $ANTHROPIC_API_KEY

# Alternative Ollama configuration (commented out)
# llm:
#   provider: ollama
#   model: qwen3:32b      # Use the model you downloaded
#   maxIterations: 50
#   baseURL: $OLLAMA_BASE_URL

# System prompt - User customizable
# This is the main static provider for users who want a simple prompt
# Make sure your custom prompt format follows the exact format of the example above
systemPrompt:
  enabled: true
  content: |
    You are an AI programming assistant focused on coding and reasoning tasks. You excel at:
    - Writing clean, efficient code
    - Debugging and problem-solving
    - Code review and optimization
    - Explaining complex technical concepts
    - Reasoning through programming challenges


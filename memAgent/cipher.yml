# describes the mcp servers to use
mcpServers:
  filesystem:
    type: stdio
    command: npx
    args:
      - -y
      - "@modelcontextprotocol/server-filesystem"
      - .

# describes the llm configuration
# OpenRouter configuration (commented out)
# llm:
#   provider: openrouter
#   model: o4-mini  
#   apiKey: $OPENROUTER_API_KEY
#   maxIterations: 50

# Ollama configuration - Using larger model for better performance
llm:
  provider: ollama
  model: qwen3:32b                   # 32B model for excellent performance with tool calling
  maxIterations: 50
  baseURL: $OLLAMA_BASE_URL 

# System Prompt
systemPrompt: "You are a helpful AI assistant with memory capabilities running on Ollama. You have access to local files and can perform various tasks using the available tools. You are powered by a large language model that excels at reasoning and tool usage."
